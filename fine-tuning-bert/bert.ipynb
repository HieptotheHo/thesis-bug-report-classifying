{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT model for classifying bug ticket severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hohuu\\miniconda3\\envs\\testing-tf-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "\n",
    "# BERT\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "# progress bar displayed while training\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>summary</th>\n",
       "      <th>severity</th>\n",
       "      <th>keywords</th>\n",
       "      <th>status</th>\n",
       "      <th>description</th>\n",
       "      <th>days_distance</th>\n",
       "      <th>created_year</th>\n",
       "      <th>updated_year</th>\n",
       "      <th>created_month</th>\n",
       "      <th>...</th>\n",
       "      <th>created_trimester</th>\n",
       "      <th>updated_trimester</th>\n",
       "      <th>created_weekday</th>\n",
       "      <th>updated_weekday</th>\n",
       "      <th>created_day_of_trimester</th>\n",
       "      <th>updated_day_of_trimester</th>\n",
       "      <th>assignee_freq</th>\n",
       "      <th>reporter_freq</th>\n",
       "      <th>product_freq</th>\n",
       "      <th>component_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>remove INTL_ConvertCharset because it is unused</td>\n",
       "      <td>2</td>\n",
       "      <td>defect</td>\n",
       "      <td>0</td>\n",
       "      <td>Created attachment 384671 remove INTL_ConvertC...</td>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>51</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fix compiler warnings in c-sdk/ldap</td>\n",
       "      <td>2</td>\n",
       "      <td>defect</td>\n",
       "      <td>0</td>\n",
       "      <td>Created attachment 384672 changes comm-central...</td>\n",
       "      <td>103</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Existing tab is overwritten when opening new m...</td>\n",
       "      <td>2</td>\n",
       "      <td>defect</td>\n",
       "      <td>1</td>\n",
       "      <td>User-Agent: Mozilla/5.0 (Windows; U; Windows N...</td>\n",
       "      <td>4857</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>10</td>\n",
       "      <td>0.179916</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.026742</td>\n",
       "      <td>0.001595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Compose window will not open when using \"send ...</td>\n",
       "      <td>3</td>\n",
       "      <td>defect</td>\n",
       "      <td>0</td>\n",
       "      <td>User-Agent: Mozilla/5.0 (Windows; U; Windows N...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>0.179916</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.026742</td>\n",
       "      <td>0.002188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>drag of message from Search Messages fails wit...</td>\n",
       "      <td>3</td>\n",
       "      <td>regression, defect</td>\n",
       "      <td>0</td>\n",
       "      <td>Error: GetSelectedMessages is not defined Sour...</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84</td>\n",
       "      <td>75</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>0.003530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            summary  severity  \\\n",
       "0           0    remove INTL_ConvertCharset because it is unused         2   \n",
       "1           1                fix compiler warnings in c-sdk/ldap         2   \n",
       "2           2  Existing tab is overwritten when opening new m...         2   \n",
       "3           3  Compose window will not open when using \"send ...         3   \n",
       "4           4  drag of message from Search Messages fails wit...         3   \n",
       "\n",
       "             keywords  status  \\\n",
       "0              defect       0   \n",
       "1              defect       0   \n",
       "2              defect       1   \n",
       "3              defect       0   \n",
       "4  regression, defect       0   \n",
       "\n",
       "                                         description  days_distance  \\\n",
       "0  Created attachment 384671 remove INTL_ConvertC...             58   \n",
       "1  Created attachment 384672 changes comm-central...            103   \n",
       "2  User-Agent: Mozilla/5.0 (Windows; U; Windows N...           4857   \n",
       "3  User-Agent: Mozilla/5.0 (Windows; U; Windows N...              0   \n",
       "4  Error: GetSelectedMessages is not defined Sour...             82   \n",
       "\n",
       "   created_year  updated_year  created_month  ...  created_trimester  \\\n",
       "0             9             9              6  ...                  2   \n",
       "1             9             9              6  ...                  2   \n",
       "2             9            22              6  ...                  2   \n",
       "3             9             9              6  ...                  2   \n",
       "4             9             9              6  ...                  2   \n",
       "\n",
       "   updated_trimester  created_weekday  updated_weekday  \\\n",
       "0                  3                5                4   \n",
       "1                  4                5                3   \n",
       "2                  4                5                1   \n",
       "3                  2                5                5   \n",
       "4                  3                5                3   \n",
       "\n",
       "   created_day_of_trimester  updated_day_of_trimester  assignee_freq  \\\n",
       "0                        84                        51       0.001626   \n",
       "1                        84                         4       0.001626   \n",
       "2                        84                        10       0.179916   \n",
       "3                        84                        84       0.179916   \n",
       "4                        84                        75       0.000446   \n",
       "\n",
       "   reporter_freq  product_freq  component_freq  \n",
       "0       0.002184      0.005565        0.000268  \n",
       "1       0.002184      0.000131        0.000113  \n",
       "2       0.000029      0.026742        0.001595  \n",
       "3       0.000014      0.026742        0.002188  \n",
       "4       0.001049      0.005565        0.003530  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"bug_tickets.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data.shape\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>remove INTL_ConvertCharset because it is unuse...</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fix compiler warnings in c-sdk/ldap defect Cre...</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Existing tab is overwritten when opening new m...</td>\n",
       "      <td>minor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compose window will not open when using \"send ...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drag of message from Search Messages fails wit...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70805</th>\n",
       "      <td>Unable to download programmatically from the T...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70806</th>\n",
       "      <td>Exception in diff view defect As I now grep th...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70807</th>\n",
       "      <td>Focused item of autocomplete menu in diff view...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70808</th>\n",
       "      <td>_libpq_pathname in postgres_backend.py should ...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70809</th>\n",
       "      <td>convert_db fails if both spamfilter and tags p...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70810 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text severity\n",
       "0      remove INTL_ConvertCharset because it is unuse...    minor\n",
       "1      fix compiler warnings in c-sdk/ldap defect Cre...    minor\n",
       "2      Existing tab is overwritten when opening new m...    minor\n",
       "3      Compose window will not open when using \"send ...   normal\n",
       "4      drag of message from Search Messages fails wit...   normal\n",
       "...                                                  ...      ...\n",
       "70805  Unable to download programmatically from the T...   normal\n",
       "70806  Exception in diff view defect As I now grep th...   normal\n",
       "70807  Focused item of autocomplete menu in diff view...   normal\n",
       "70808  _libpq_pathname in postgres_backend.py should ...   normal\n",
       "70809  convert_db fails if both spamfilter and tags p...   normal\n",
       "\n",
       "[70810 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping = {\n",
    "    1: 'feature',\n",
    "    2: 'minor',\n",
    "    3: 'normal',\n",
    "    0: 'critical',\n",
    "}\n",
    "data['severity'] = data['severity'].map(label_mapping)\n",
    "\n",
    "data['text'] = (\n",
    "    data['summary'].fillna('') + ' ' +\n",
    "    data['keywords'].fillna('') + ' ' +\n",
    "    data['description'].fillna('')\n",
    ")\n",
    "data[['text', 'severity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "severity\n",
       "normal      40783\n",
       "minor       13048\n",
       "feature     11169\n",
       "critical     5810\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['severity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hohuu\\AppData\\Local\\Temp\\ipykernel_12644\\1622042076.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['label'] = data['severity'].replace(label_dict)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severity</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">critical</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>4648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">feature</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">minor</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>10438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">normal</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>32627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>8156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           text\n",
       "severity label data_type       \n",
       "critical 3     train       4648\n",
       "               val         1162\n",
       "feature  0     train       8935\n",
       "               val         2234\n",
       "minor    1     train      10438\n",
       "               val         2610\n",
       "normal   2     train      32627\n",
       "               val         8156"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = data['severity'].unique();\n",
    "label_dict = {'feature':0, 'minor':1, 'normal':2, 'critical':3}\n",
    "\n",
    "data['label'] = data['severity'].replace(label_dict)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data.index.values,\n",
    "                                                  data['label'].values,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=data['label'].values)\n",
    "\n",
    "data['data_type'] = ['not_set']*data.shape[0]\n",
    "\n",
    "data.loc[X_train, 'data_type'] = 'train'\n",
    "data.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "data = data[['severity','label','data_type','text']]\n",
    "data.groupby(['severity', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\hohuu\\miniconda3\\envs\\testing-tf-gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    data[data['data_type']=='train'].text.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    data[data['data_type']=='val'].text.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(data[data['data_type']=='train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(data[data['data_type']=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train,\n",
    "                              sampler=RandomSampler(dataset_train),\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val,\n",
    "                                   sampler=SequentialSampler(dataset_val),\n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\hohuu\\miniconda3\\envs\\testing-tf-gpu\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=len(label_dict),output_attentions=False,output_hidden_states=False)\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    print(classification_report(preds_flat, labels_flat, target_names=label_dict))\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "    return np.sum(preds_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.6687769489174924\n",
      "Training epoch took: 0:58:11\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     feature       0.73      0.78      0.76      2089\n",
      "       minor       0.53      0.60      0.56      2323\n",
      "      normal       0.91      0.82      0.86      9104\n",
      "    critical       0.45      0.80      0.57       646\n",
      "\n",
      "    accuracy                           0.78     14162\n",
      "   macro avg       0.66      0.75      0.69     14162\n",
      "weighted avg       0.80      0.78      0.79     14162\n",
      "\n",
      "Class: feature\n",
      "Accuracy: 1639/2234\n",
      "\n",
      "Class: minor\n",
      "Accuracy: 1390/2610\n",
      "\n",
      "Class: normal\n",
      "Accuracy: 7456/8156\n",
      "\n",
      "Class: critical\n",
      "Accuracy: 518/1162\n",
      "\n",
      "Validation loss: 0.6314546233577683\n",
      "Validation Accuracy: 0.776938285552888\n",
      "Validation epoch took: 0:03:26\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.5630321384233528\n",
      "Training epoch took: 0:57:31\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     feature       0.70      0.84      0.76      1868\n",
      "       minor       0.56      0.60      0.58      2430\n",
      "      normal       0.92      0.82      0.87      9136\n",
      "    critical       0.49      0.79      0.61       728\n",
      "\n",
      "    accuracy                           0.78     14162\n",
      "   macro avg       0.67      0.76      0.70     14162\n",
      "weighted avg       0.81      0.78      0.79     14162\n",
      "\n",
      "Class: feature\n",
      "Accuracy: 1560/2234\n",
      "\n",
      "Class: minor\n",
      "Accuracy: 1467/2610\n",
      "\n",
      "Class: normal\n",
      "Accuracy: 7494/8156\n",
      "\n",
      "Class: critical\n",
      "Accuracy: 573/1162\n",
      "\n",
      "Validation loss: 0.5997036988422823\n",
      "Validation Accuracy: 0.7833639316480723\n",
      "Validation epoch took: 0:03:35\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.4470862317265109\n",
      "Training epoch took: 0:59:15\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     feature       0.70      0.84      0.76      1855\n",
      "       minor       0.53      0.62      0.57      2220\n",
      "      normal       0.92      0.81      0.86      9244\n",
      "    critical       0.53      0.73      0.62       843\n",
      "\n",
      "    accuracy                           0.78     14162\n",
      "   macro avg       0.67      0.75      0.70     14162\n",
      "weighted avg       0.81      0.78      0.79     14162\n",
      "\n",
      "Class: feature\n",
      "Accuracy: 1560/2234\n",
      "\n",
      "Class: minor\n",
      "Accuracy: 1379/2610\n",
      "\n",
      "Class: normal\n",
      "Accuracy: 7519/8156\n",
      "\n",
      "Class: critical\n",
      "Accuracy: 617/1162\n",
      "\n",
      "Validation loss: 0.6625351852854152\n",
      "Validation Accuracy: 0.7820223132325943\n",
      "Validation epoch took: 0:03:48\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.31130739742903957\n",
      "Training epoch took: 0:59:39\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     feature       0.68      0.82      0.75      1856\n",
      "       minor       0.58      0.57      0.57      2663\n",
      "      normal       0.88      0.83      0.85      8679\n",
      "    critical       0.56      0.67      0.61       964\n",
      "\n",
      "    accuracy                           0.77     14162\n",
      "   macro avg       0.68      0.72      0.70     14162\n",
      "weighted avg       0.78      0.77      0.77     14162\n",
      "\n",
      "Class: feature\n",
      "Accuracy: 1529/2234\n",
      "\n",
      "Class: minor\n",
      "Accuracy: 1510/2610\n",
      "\n",
      "Class: normal\n",
      "Accuracy: 7178/8156\n",
      "\n",
      "Class: critical\n",
      "Accuracy: 650/1162\n",
      "\n",
      "Validation loss: 0.7604679271788982\n",
      "Validation Accuracy: 0.7673351221578874\n",
      "Validation epoch took: 0:03:38\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.19815438093094218\n",
      "Training epoch took: 0:57:50\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     feature       0.70      0.78      0.74      2007\n",
      "       minor       0.57      0.57      0.57      2590\n",
      "      normal       0.87      0.83      0.85      8595\n",
      "    critical       0.57      0.69      0.62       970\n",
      "\n",
      "    accuracy                           0.77     14162\n",
      "   macro avg       0.68      0.72      0.70     14162\n",
      "weighted avg       0.77      0.77      0.77     14162\n",
      "\n",
      "Class: feature\n",
      "Accuracy: 1573/2234\n",
      "\n",
      "Class: minor\n",
      "Accuracy: 1488/2610\n",
      "\n",
      "Class: normal\n",
      "Accuracy: 7128/8156\n",
      "\n",
      "Class: critical\n",
      "Accuracy: 665/1162\n",
      "\n",
      "Validation loss: 0.897299155552194\n",
      "Validation Accuracy: 0.7664171727157181\n",
      "Validation epoch took: 0:03:30\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.12852609626176312\n",
      "Training epoch took: 0:58:54\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     feature       0.69      0.79      0.74      1955\n",
      "       minor       0.56      0.57      0.57      2587\n",
      "      normal       0.88      0.82      0.85      8674\n",
      "    critical       0.56      0.69      0.62       946\n",
      "\n",
      "    accuracy                           0.76     14162\n",
      "   macro avg       0.67      0.72      0.69     14162\n",
      "weighted avg       0.77      0.76      0.77     14162\n",
      "\n",
      "Class: feature\n",
      "Accuracy: 1549/2234\n",
      "\n",
      "Class: minor\n",
      "Accuracy: 1474/2610\n",
      "\n",
      "Class: normal\n",
      "Accuracy: 7153/8156\n",
      "\n",
      "Class: critical\n",
      "Accuracy: 651/1162\n",
      "\n",
      "Validation loss: 1.0148873300301535\n",
      "Validation Accuracy: 0.7645106623358283\n",
      "Validation epoch took: 0:03:34\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0837817540198867\n",
      "Training epoch took: 0:58:26\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     feature       0.71      0.76      0.74      2103\n",
      "       minor       0.55      0.58      0.57      2470\n",
      "      normal       0.88      0.83      0.85      8666\n",
      "    critical       0.55      0.69      0.61       923\n",
      "\n",
      "    accuracy                           0.77     14162\n",
      "   macro avg       0.67      0.72      0.69     14162\n",
      "weighted avg       0.78      0.77      0.77     14162\n",
      "\n",
      "Class: feature\n",
      "Accuracy: 1595/2234\n",
      "\n",
      "Class: minor\n",
      "Accuracy: 1440/2610\n",
      "\n",
      "Class: normal\n",
      "Accuracy: 7165/8156\n",
      "\n",
      "Class: critical\n",
      "Accuracy: 640/1162\n",
      "\n",
      "Validation loss: 1.2204877968783578\n",
      "Validation Accuracy: 0.7654286117779975\n",
      "Validation epoch took: 0:03:32\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.054053249243607764\n",
      "Training epoch took: 1:00:10\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     feature       0.72      0.76      0.74      2096\n",
      "       minor       0.54      0.58      0.56      2426\n",
      "      normal       0.88      0.82      0.85      8787\n",
      "    critical       0.53      0.72      0.61       853\n",
      "\n",
      "    accuracy                           0.76     14162\n",
      "   macro avg       0.67      0.72      0.69     14162\n",
      "weighted avg       0.78      0.76      0.77     14162\n",
      "\n",
      "Class: feature\n",
      "Accuracy: 1600/2234\n",
      "\n",
      "Class: minor\n",
      "Accuracy: 1400/2610\n",
      "\n",
      "Class: normal\n",
      "Accuracy: 7210/8156\n",
      "\n",
      "Class: critical\n",
      "Accuracy: 615/1162\n",
      "\n",
      "Validation loss: 1.3087178790483873\n",
      "Validation Accuracy: 0.7643694393447253\n",
      "Validation epoch took: 0:03:27\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.03450054061748316\n",
      "Training epoch took: 0:57:19\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     feature       0.71      0.76      0.73      2069\n",
      "       minor       0.53      0.59      0.56      2325\n",
      "      normal       0.88      0.82      0.85      8770\n",
      "    critical       0.56      0.66      0.61       998\n",
      "\n",
      "    accuracy                           0.76     14162\n",
      "   macro avg       0.67      0.71      0.69     14162\n",
      "weighted avg       0.78      0.76      0.77     14162\n",
      "\n",
      "Class: feature\n",
      "Accuracy: 1580/2234\n",
      "\n",
      "Class: minor\n",
      "Accuracy: 1373/2610\n",
      "\n",
      "Class: normal\n",
      "Accuracy: 7214/8156\n",
      "\n",
      "Class: critical\n",
      "Accuracy: 656/1162\n",
      "\n",
      "Validation loss: 1.4417113799380665\n",
      "Validation Accuracy: 0.7642282163536224\n",
      "Validation epoch took: 0:03:23\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.023557862836267943\n",
      "Training epoch took: 0:56:51\n",
      "\n",
      "Running Validation...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     feature       0.71      0.78      0.74      2018\n",
      "       minor       0.55      0.58      0.57      2458\n",
      "      normal       0.88      0.82      0.85      8734\n",
      "    critical       0.56      0.68      0.61       952\n",
      "\n",
      "    accuracy                           0.77     14162\n",
      "   macro avg       0.67      0.72      0.69     14162\n",
      "weighted avg       0.78      0.77      0.77     14162\n",
      "\n",
      "Class: feature\n",
      "Accuracy: 1575/2234\n",
      "\n",
      "Class: minor\n",
      "Accuracy: 1433/2610\n",
      "\n",
      "Class: normal\n",
      "Accuracy: 7191/8156\n",
      "\n",
      "Class: critical\n",
      "Accuracy: 646/1162\n",
      "\n",
      "Validation loss: 1.4985755314940652\n",
      "Validation Accuracy: 0.7657816692557549\n",
      "Validation epoch took: 0:03:26\n",
      "\n",
      "Training complete!\n",
      "Total training took 10:19:32 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "seed_val = 42\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "training_stats = []\n",
    "\n",
    "import time\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\")\n",
    "    print(f'======== Epoch {epoch+1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch+1), leave=False, disable=False)\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.train()\n",
    "    for batch in progress_bar:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        \n",
    "        model.zero_grad()\n",
    "     \n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        logits = outputs[1]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "########################################\n",
    "\n",
    "        labels = inputs['labels'].cpu().numpy()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'training_loss': '{:.3f}'.format(loss.item() / len(batch))\n",
    "        })\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(dataloader_train)\n",
    "    print(\"\")\n",
    "    print(f'Average training loss: {avg_train_loss}')\n",
    "    print(f'Training epoch took: {format_time(time.time()-t0)}')\n",
    "    print(\"\")\n",
    "    print('Running Validation...')\n",
    "    t0 = time.time()\n",
    "    \n",
    "    loss_val, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_acc = flat_accuracy(predictions, true_vals)\n",
    "    print(f'Validation loss: {loss_val}')\n",
    "    print(f'Validation Accuracy: {val_acc}')\n",
    "    print(f'Validation epoch took: {format_time(time.time()-t0)}')\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
    "\n",
    "    # save training stats at last\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': loss_val,\n",
    "            'Valid. Accur.': val_acc,\n",
    "            'Training Time': format_time(time.time()-t0),\n",
    "            'Validation Time': format_time(time.time()-t0)\n",
    "        }\n",
    "    )\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing-tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
